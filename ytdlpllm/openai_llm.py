"""Implementation of LLMInterface for OpenAI LLM API."""
from openai import OpenAI, OpenAIError
import sys
from typing import List, Dict
from ytdlpllm.llm_interface import LLMInterface


class OpenAILLMInterface(LLMInterface):
    """Implementation of LLMInterface for OpenAI LLM API."""

    def __init__(self, model_string: str, base_url: str, api_key: str):
        """Initialize OpenAI API connection and create message history.

        Args:
            model_string (str): The specific model to be used in API.
            base_url (str): The base URL for the OpenAI API.
            api_key (str): The API key for the OpenAI API.
        """
        self._model_string = model_string

        try:
            self.client = OpenAI(base_url=base_url, api_key=api_key)
        except OpenAIError as e:
            print(
                f"OpenAI API Unable to initialize, likely due to missing API Key environment Variable: {e}",  # noqa: E501
                file=sys.stderr,
            )
            exit(1)

        self.history: List[Dict[str, str]] = []

    def add_system_prompt(self, prompt: str):
        """Add a system prompt to the model's context.

        Uses the format outlined in OpenAI's LLM API.

        Args:
            prompt (str): The system prompt text to be added.
        """
        self.history.append({"role": "system", "content": prompt})

    def add_assistant_prompt(self, prompt: str):
        """Add an assistant prompt to the model's context.

        Uses the format outlined in OpenAI's LLM API.

        Args:
            prompt (str): The assistant prompt text to be added.
        """
        self.history.append({"role": "assistant", "content": prompt})

    def add_user_prompt(self, prompt: str):
        """Add a user prompt to the model's context.

        Uses the format outlined in OpenAI's LLM API.

        Args:
            prompt (str): The user prompt text to be added.
        """
        self.history.append({"role": "user", "content": prompt})

    def invoke_model(self) -> str:
        """Invoke the OpenAI LLM Chat API to obtain response.

        Species the return type to be a json object. Also
        specifies the specific API model type to use.

        Returns:
            str: The output generated by the language model.
        """
        response = self.client.chat.completions.create(
            model=self._model_string,
            response_format={"type": "json_object"},
            messages=self.history,
            temperature=0.2,
            top_p=0.2,
        )

        # Extract the LLM's decision from the response
        raw_json_string = response.choices[0].message.content

        return raw_json_string